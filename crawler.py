"""
ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
- êµ­ë‚´: ë„¤ì´ë²„ ë‰´ìŠ¤ API
- ê¸€ë¡œë²Œ: Google News RSS
- ë³¸ë¬¸: Newspaper3k
"""

import requests
import time
import pandas as pd
import urllib.parse
import xml.etree.ElementTree as ET
import re
from datetime import datetime, timedelta
from newspaper import Article
from config import CRAWL_CONFIG

def is_korean(text):
    """í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    return bool(re.search('[ê°€-í£]', text))


def crawl_naver(keyword, client_id, client_secret, display=20):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ API ê²€ìƒ‰
    """
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": keyword,
        "display": display,
        "sort": "date",
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        articles = []
        for item in data.get("items", []):
            # HTML íƒœê·¸ ì œê±°
            title = re.sub(r'<[^>]+>', '', item.get("title", ""))
            snippet = re.sub(r'<[^>]+>', '', item.get("description", ""))
            
            articles.append({
                "title": title,
                "link": item.get("originallink") or item.get("link", ""),
                "snippet": snippet,
                "date": item.get("pubDate", ""),
                "source": "ë„¤ì´ë²„ë‰´ìŠ¤",
                "keyword": keyword,
            })
        
        return articles
        
    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤íŒ¨ [{keyword}]: {e}")
        return []


def crawl_google_rss(keyword, num=20, lang="en"):
    """
    Google News RSS ê²€ìƒ‰ (ë¬´ë£Œ ë¬´ì œí•œ)
    """
    encoded_keyword = urllib.parse.quote(keyword)
    
    if lang == "ko":
        url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
    else:
        url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=en&gl=US&ceid=US:en"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        root = ET.fromstring(response.content)
        articles = []
        
        for item in root.findall(".//item")[:num]:
            title = item.findtext("title", "")
            link = item.findtext("link", "")
            pub_date = item.findtext("pubDate", "")
            
            # ì†ŒìŠ¤ ì¶”ì¶œ (ì œëª©ì—ì„œ " - ì†ŒìŠ¤ëª…" íŒ¨í„´)
            source = "Google News"
            if " - " in title:
                parts = title.rsplit(" - ", 1)
                title = parts[0]
                source = parts[1] if len(parts) > 1 else "Google News"
            
            articles.append({
                "title": title,
                "link": link,
                "snippet": "",  # RSSëŠ” snippet ì—†ìŒ
                "date": pub_date,
                "source": source,
                "keyword": keyword,
            })
        
        return articles
        
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ RSS ê²€ìƒ‰ ì‹¤íŒ¨ [{keyword}]: {e}")
        return []


def fetch_full_article(url):
    """
    Newspaper3kë¡œ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§
    """
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text[:3000]  # ìµœëŒ€ 3000ì
    except Exception as e:
        return ""


def parse_date(date_str):
    """
    ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±
    """
    from datetime import datetime
    import re
    
    if not date_str:
        return None
    
    date_str = str(date_str).strip()
    
    try:
        # "2 hours ago", "3ì¼ ì „" ë“± ìƒëŒ€ ì‹œê°„
        date_lower = date_str.lower()
        
        if "hour" in date_lower or "ì‹œê°„" in date_lower or "ë¶„" in date_lower or "minute" in date_lower:
            return datetime.now()
        
        if "day" in date_lower or "ì¼ ì „" in date_str:
            nums = re.findall(r'\d+', date_str)
            days = int(nums[0]) if nums else 1
            return datetime.now() - timedelta(days=days)
        
        if "week" in date_lower or "ì£¼" in date_str:
            nums = re.findall(r'\d+', date_str)
            weeks = int(nums[0]) if nums else 1
            return datetime.now() - timedelta(days=weeks * 7)
        
        if "month" in date_lower or "ë‹¬" in date_str or "ê°œì›”" in date_str:
            nums = re.findall(r'\d+', date_str)
            months = int(nums[0]) if nums else 1
            return datetime.now() - timedelta(days=months * 30)
        
        # RFC 2822 í˜•ì‹: "Tue, 14 Oct 2025 11:40:00 +0900"
        try:
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except:
            pass
        
        # ISO í˜•ì‹: "2025-10-14"
        try:
            return datetime.strptime(date_str[:10], "%Y-%m-%d")
        except:
            pass
        
        # í•œê¸€ í˜•ì‹: "2025.10.14"
        try:
            return datetime.strptime(date_str[:10], "%Y.%m.%d")
        except:
            pass
        
    except:
        pass
    
    return None


def filter_by_date(df, days_ago):
    """
    ë‚ ì§œ ê¸°ì¤€ í•„í„°ë§
    """
    if df.empty:
        return df
    
    df = df.copy()
    cutoff_date = datetime.now() - timedelta(days=days_ago)
    
    def is_within_range(date_str):
        parsed = parse_date(date_str)
        if parsed is None:
            return False
        
        # timezone ì œê±° (naiveë¡œ í†µì¼)
        if parsed.tzinfo is not None:
            parsed = parsed.replace(tzinfo=None)
        
        return parsed >= cutoff_date
    
    before_count = len(df)
    df = df[df["date"].apply(is_within_range)]
    df = df.reset_index(drop=True)
    after_count = len(df)
    
    print(f"ğŸ“… ë‚ ì§œ í•„í„°: {before_count}ê±´ â†’ {after_count}ê±´ ({days_ago}ì¼ ì´ë‚´)")
    
    return df

def crawl_all(keywords_dict, naver_id, naver_secret, days_ago=7):
    """
    ì „ì²´ í‚¤ì›Œë“œ í¬ë¡¤ë§
    - í•œê¸€ í‚¤ì›Œë“œ â†’ ë„¤ì´ë²„ API
    - ì˜ë¬¸ í‚¤ì›Œë“œ â†’ Google RSS
    """
    all_articles = []
    delay = CRAWL_CONFIG.get("delay", 0.5)
    naver_display = CRAWL_CONFIG.get("naver_display", 20)
    google_num = CRAWL_CONFIG.get("google_num", 20)
    
    total = sum(len(kws) for kws in keywords_dict.values())
    current = 0
    
    for category, keywords in keywords_dict.items():
        for keyword in keywords:
            current += 1
            
            if is_korean(keyword):
                # í•œê¸€ â†’ ë„¤ì´ë²„
                print(f"ğŸ” [{current}/{total}] ë„¤ì´ë²„: {keyword}")
                articles = crawl_naver(keyword, naver_id, naver_secret, naver_display)
            else:
                # ì˜ë¬¸ â†’ êµ¬ê¸€ RSS
                print(f"ğŸ” [{current}/{total}] êµ¬ê¸€: {keyword}")
                articles = crawl_google_rss(keyword, google_num, lang="en")
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ê°€
            for article in articles:
                article["category"] = category
            
            all_articles.extend(articles)
            time.sleep(delay)
    
# DataFrame ë³€í™˜
    df = pd.DataFrame(all_articles)
    
    # ë§í¬ ê¸°ì¤€ ì¤‘ë³µ ì œê±° (1ì°¨)
    if not df.empty:
        df = df.drop_duplicates(subset=["link"], keep="first")
        df = df.reset_index(drop=True)
        
        # ë‚ ì§œ í•„í„°ë§
        df = filter_by_date(df, days_ago)
        
        print(f"âœ… ì´ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ (ë‚ ì§œ í•„í„° ì ìš©)")
    
    return df


def crawl_with_fulltext(df):
    """
    DataFrameì˜ ëª¨ë“  ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
    """
    if df.empty:
        return df
    
    df = df.copy()
    full_texts = []
    
    total = len(df)
    for idx, row in df.iterrows():
        print(f"ğŸ“„ ë³¸ë¬¸ ìˆ˜ì§‘ [{idx+1}/{total}] {row['title'][:40]}...")
        full_text = fetch_full_article(row["link"])
        full_texts.append(full_text)
        time.sleep(0.3)
    
    df["full_text"] = full_texts
    
    # snippetì´ ë¹„ì–´ìˆìœ¼ë©´ full_textë¡œ ëŒ€ì²´
    df["snippet"] = df.apply(
        lambda x: x["full_text"][:500] if not x["snippet"] and x["full_text"] else x["snippet"],
        axis=1
    )
    
    print(f"âœ… ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {total}ê±´")
    return df